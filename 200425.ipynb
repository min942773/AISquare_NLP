{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Open\n",
    "1. Review\n",
    "2. Topic\n",
    "\n",
    "    2.1 이전 시간 source 분석\n",
    "    \n",
    "    2.2 어텐션 매커니즘\n",
    "    \n",
    "    2.3 사후 평가 및 실습 평가 / 설문 평가\n",
    "    \n",
    "    2.4 과정 마무리\n",
    "    \n",
    "3. Q & A\n",
    "4. Close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션 메커니즘(Attention Mechanism)\n",
    "\n",
    "- seq2seq 모델의 디코더에서 출력 단어를 예측하는데 매 시점(time step)마다, 인코더에서 전체 입력 문장을 다시 한 번 참고한다.\n",
    "- 전체 입력 문장을 전부 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 보게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 함수(Attention Function)\n",
    "\n",
    "Attention(Q, K, B) = Attention, Value\n",
    "\n",
    "- 어텐션 함수는 주어진 Q(Query)에 대해서 모든 K(Key)와의 유사도를 각각 구하고, 구해낸 유사도를 key와 맵핑되어 있는 각각의 V(Value)에 반영해 준다. 그리고 유사도가 반영된 V(Value)를 모두 더해서 리턴한다. 리턴된 값은 어텐션 값(Attention Value)이라 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 닷-프로덕트 어텐션(Dot-product Attention) : 어텐션 함수가 벡터의 전치와 내적을 이용하여 어텐션 값을 구하는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스포머(Transformer) : 인코더와 디코더를 어텐션 메커니즘으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
