{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. open\n",
    "1. review\n",
    "2. topic<br>\n",
    "    2.1 토픽 모델링 (Topic Modeling)<br>\n",
    "3. Q & A\n",
    "4. next\n",
    "5. close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 카운터 기반 단어 표현\n",
    "- BoW(Bag of Words) - 하나의 문서에 대한 빈도수로 단어 표현 (vector형태, 1차원)\n",
    "- DTM(Document-Term Matrix) - 여러 문서에 대한 빈도수로 단어 표현 (matrix형태, 2차원)\n",
    "- TF-IDF(Term Frequency-Inverse Document Frequency) : DTM에 가중치를 적용해서 단어 표현\n",
    "\n",
    "#### 문서 유사도(Document Similarity)\n",
    "- Cosine Similarity : 문서의 방향 판단\n",
    "- Euclidean distance : 문서의 거리를 통한 판단\n",
    "- Jaccard similarity : 합집합에서 교집합의 비율에 따른 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도를 이용한 추천 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 수집한 data를 이용하여 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 119050, 'name': 'Grumpy Old Men Collect...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15602</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>en</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>False</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31357</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>en</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>False</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 96871, 'name': 'Father of the Bride Col...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11862</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>en</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>76578911.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>False</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
       "3  False                                                NaN  16000000   \n",
       "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                               homepage     id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
       "1                                   NaN   8844  tt0113497                en   \n",
       "2                                   NaN  15602  tt0113228                en   \n",
       "3                                   NaN  31357  tt0114885                en   \n",
       "4                                   NaN  11862  tt0113041                en   \n",
       "\n",
       "                original_title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  ... release_date  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  ...   1995-10-30   \n",
       "1  When siblings Judy and Peter discover an encha...  ...   1995-12-15   \n",
       "2  A family wedding reignites the ancient feud be...  ...   1995-12-22   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  ...   1995-12-22   \n",
       "4  Just when George Banks has recovered from his ...  ...   1995-02-10   \n",
       "\n",
       "       revenue runtime                                   spoken_languages  \\\n",
       "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
       "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "\n",
       "     status                                            tagline  \\\n",
       "0  Released                                                NaN   \n",
       "1  Released          Roll the dice and unleash the excitement!   \n",
       "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Released  Friends are the people who let you be yourself...   \n",
       "4  Released  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                         title  video vote_average vote_count  \n",
       "0                    Toy Story  False          7.7     5415.0  \n",
       "1                      Jumanji  False          6.9     2413.0  \n",
       "2             Grumpier Old Men  False          6.5       92.0  \n",
       "3            Waiting to Exhale  False          6.1       34.0  \n",
       "4  Father of the Bride Part II  False          5.7      173.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('the-movies-dataset/movies_metadata.csv', low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20000개의 데이터로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'ovewview' 변수에 null data 확인 및 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['overview'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['overview'] = data['overview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['overview'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF  수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(data['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 47487)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코사인 유사도를 이용한 유사도 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영화 타이틀과 인덱스를 가진 테이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "Toy Story                      0\n",
      "Jumanji                        1\n",
      "Grumpier Old Men               2\n",
      "Waiting to Exhale              3\n",
      "Father of the Bride Part II    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "indices = pd.Series(data.index, index=data['title']).drop_duplicates()\n",
    "print(indices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "idx = indices['Jumanji']\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선택한 영화에 대한 추천을 수행하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = indices[title] # 선택한 영화에 대한 인덱스\n",
    "    sim_scores = list(enumerate(cosine_sim[idx])) # 모든 영화에 대한 해당 영화와의 유사도\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    return data['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12481                            The Dark Knight\n",
       "150                               Batman Forever\n",
       "1328                              Batman Returns\n",
       "15511                 Batman: Under the Red Hood\n",
       "585                                       Batman\n",
       "9230          Batman Beyond: Return of the Joker\n",
       "18035                           Batman: Year One\n",
       "19792    Batman: The Dark Knight Returns, Part 1\n",
       "3095                Batman: Mask of the Phantasm\n",
       "10122                              Batman Begins\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15348               Toy Story 3\n",
       "2997                Toy Story 2\n",
       "10301    The 40 Year Old Virgin\n",
       "8327                  The Champ\n",
       "1071      Rebel Without a Cause\n",
       "11399    For Your Consideration\n",
       "1932                  Condorman\n",
       "3057            Man on the Moon\n",
       "485                      Malice\n",
       "11606              Factory Girl\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Toy Story')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic은 한국어로 주제라는 의미\n",
    "\n",
    "- Topic Modeling은 기계학습 및 자연어처리 분야에서 Topic이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나로, text 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 Text Mining 기법\n",
    "\n",
    "- BoW에 기반한 DTM 또는 TF-IDF는 단어의 빈도수를 이용한 수치화 방법을 활용, 단점은 의미를 고려하지 못한다는 점\n",
    "\n",
    "- DTM의 잠재된(Latent) 의미를 이끌어내는 방법 : LSA(Latent Sematic Analysis), LDA(Latent Dirichlet Allocation)\n",
    "\n",
    "- LSA(Latent Sematic Analysis) : 차원 축소(Dimension reduction) 기법 중 하나인 특이값 분해(Singular Value Decomposion, SVD)을 이용한 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특이값 분해(Singular Value Decomposition, SVD)\n",
    "\n",
    "- 3개의 행렬에 대한 곱으로 분해\n",
    "\n",
    "$A = USV^T$<br>\n",
    "U : m * m 직교행렬<br>\n",
    "V : n * n 직교행렬<br>\n",
    "S : m * m 직사각 대각행렬<br>\n",
    "\n",
    "- 직교행렬 : 자신과 자신의 전치행렬의 곱 또는 이를 반대로 곱한 결과가 단위행렬이 되는 행렬\n",
    "- 대각행렬 : 주대각선을 제외한 곳의 원소가 모두 0인 행렬\n",
    "\n",
    "#### 절단된 SVD(Truncated SVD) - LSA에서 사용\n",
    "- Full SVD에서 일부 vector를 삭제시킨 SVD\n",
    "- 절단된 SVD는 대각 행렬 S의 대각 원소의 값 중에서 상위값 t개만 남게 된다. 절단된SVD를 수행하면 값의 손실이 일어나므로 기존 행렬 A를 복구할 수 없다. U행렬과 V행렬의 t열까지만 남긴다. t는 찾고자하는 토픽의 수를 반영한 하이퍼파라미터값이다.\n",
    "- 하이퍼파라미터란 사용자가 직접 값을 선택하여 성능에 영향을 주는 매개변수를 말한다.\n",
    "\n",
    "#### LSA(Latent Sematic Analysis) \n",
    "- LSA(Latent Sematic Analysis) : 기존 DTM이나 DTM에 단어의 중요도에 따른 가중치를 주었던 TF-IDF는 단어의 의미를 고려하지 못하는 단점이 있는데 LSA에서는 이런 단점을 해소하기 위하여 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 적용한 알고리즘\n",
    "- DTM이나 TF-IDF 행렬은 단어 의미를 고려하지 못하는 단점\n",
    "- LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 적용한 알고리즘이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "             [0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
    "             [0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
    "             [0, 1, 1, 0, 2, 0, 0, 0, 0],\n",
    "             [1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "])\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full SVD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, VT = np.linalg.svd(A, full_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U : m * m 직교행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(U.round(2))\n",
    "np.shape(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### s : m * m 대각행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.69 2.05 1.73 0.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s.round(2))\n",
    "np.shape(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.zeros((4, 9))\n",
    "S[:4, :4] = np.diag(s)\n",
    "print(S.round(2))\n",
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vt = n * x 직교행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(VT.round(2))\n",
    "np.shape(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $A = USV^T$ -> $U * S * V^T == A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, np.dot(np.dot(U, S), VT).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 절단된 SVD(Truncated SVD)를 생성\n",
    "- t값(구하려는 topic 수)을 정하고, t값에 따른 SVD를 구성\n",
    "- t = 2 (하이퍼파라미터, 임의로 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = S[:2, :2]\n",
    "print(S.round(2))\n",
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 9)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT = VT[:2, :]\n",
    "print(VT.round(2))\n",
    "np.shape(VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75]\n",
      " [-0.51  0.44]\n",
      " [-0.83 -0.49]\n",
      " [-0.   -0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = U[:, :2]\n",
    "print(U.round(2))\n",
    "np.shape(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.   -0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "A_prime = np.dot(np.dot(U, S), VT)\n",
    "print(A)\n",
    "print()\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- U는 4 x 2의 크기를 가지고 문서 개수 x 토픽 수(t) -> 의미 파악을 원하는 개수가 2, 잠재 의미를 표현하기 위한 수치화 된 각각의 문서 벡터\n",
    "- VT는 2 x 9의 크기를 가지고 토픽 수(t) x 단어의 개수 -> 단어의 잠재 의미를 표현하기 위해 수치화된 단어 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 벡터들과 단어 벡터들을 통해 다른 문서의 유사도, 다른 단어의 유사도, 단어(Query)로 부터 문서의 유사도를 구하는 것들이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA를 사용해서 문서의 수를 원하는 토픽의 수로 압축한 뒤에 각 토픽당 가장 중요한 단어 5개를 출력하는 토픽 모델링 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x : ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "tokenize_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenize_doc = tokenize_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_doc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. TF-IDF 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역 토큰화 (토큰화 작업을 역으로 수행)\n",
    "detokenize_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenize_doc[i])\n",
    "    detokenize_doc.append(t)\n",
    "    \n",
    "news_df['clean_doc'] = detokenize_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             max_features=1000,\n",
    "                             max_df=0.5,\n",
    "                             smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(svd_model.components_) # 토픽수 t X 단어 수 : 단어 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic  2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
      "Topic  3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
      "Topic  4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
      "Topic  5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
      "Topic  6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
      "Topic  7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
      "Topic  8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
      "Topic  9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
      "Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
      "Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
      "Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
      "Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
      "Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
      "Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
      "Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
      "Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
      "Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
      "Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
      "Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def get_topics(components,feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic {0:2}: {1}\".format((idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n-1:-1]]))\n",
    "        \n",
    "get_topics(svd_model.components_, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling은 문서의 집합에서 topic을 찾아내는 프로세스이다. 검색 엔진, 고객 민원 시스템 등과 같이 문서의 주제를 알아내는 일이 중요한 곳에서 사용된다.\n",
    "\n",
    "#### 잠재 디리클레 할당(Letent Dirichlet Allocation, LDA)은 topic modeling의 대표적인 알고리즘이다.\n",
    "\n",
    "#### LDA는 문서들은 topic들의 혼합으로 구성되어져 있으며, topic들은 확률 분호에 기반하여 단어들을 생성한다고 가장한다.\n",
    "\n",
    "- 데이터가 주어지면 LDA는 문서가 생성되던 과정을 역추적한다.\n",
    "\n",
    "#### LSA와 LDA의 차이\n",
    "- LSA : DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.\n",
    "- LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA를 이용한 topic modeling 전체 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) LDA를 수행하기 위해 우선 k값(t값, 하이퍼파라미터, topic 수) 결정\n",
    "2) LDA가 각 문서의 topic 분포와 각 topic 내의 단어 분폴르 추정한다.\n",
    "\n",
    "< 각 문서 topic 분포 >\n",
    "- 문서1 : 토픽 A 100%\n",
    "- 문서2 : 토픽 B 100%\n",
    "- 문서3 : 토픽 B 60%, 토픽 A 40%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA의 가정\n",
    "- LDA는 문서의 집합으로부터 어떤 topic이 존재하는지를 알아내기 위한 알고리즘이다.\n",
    "- LDA는 DTM/TF-IDF 행렬을 입력으로 하여 topic을 알아내는 알고리즘으로 단어의 순서는 고려하지 않는다.\n",
    "- LDA는 문서의 집합으로부터 topic을 뽑아낼 때 다음과 같은 가정을 염두에 두고 수행한다. \" 나는 이 문서를 작성하기 위해서 이런 주제들을 넣을거고, 이런 주제들을 위해서는 이런 단어들을 넣을거야.\"\n",
    "\n",
    "1. 문서에 사용할 단어의 개수 N을 정한다.\n",
    "2. 문서에 사용할 topic의 혼합을 확률 분포에 기반하여 결정한다.\n",
    "3. 문서에 사용할 각 단어를 다음과 같이 정한다<br>\n",
    "    3.1 topic 분포에서 topic T를 확률적으로 고른다.<br>\n",
    "    3.2 선택한 topic T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고른다.\n",
    "   \n",
    "이러한 과정을 통해 문서가 작성되었다는 가정 하에 LDA는 topic을 뽑아내기 위하여 위 과정을 역으로 추적하는 역공학(reverse engineering)을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA 수행 과정\n",
    "1. 사용자는 알고리즘에게 topic의 개수 k를 알려준다\n",
    "2. 모든 문서를 k개 중 하나의 topic에 할당한다.\n",
    "3. 이제 모든 문서의 모든 단어에 대해서 다음 사항을 반복해서 진행한다.(iteration)<br>\n",
    "    3.1 어떤 문서의 각 단어 w는 자신은 잘못된 topic에 할당되어져 있지만, 다른 단어들은 전부 올바른 topic에 할당되어져 있는 상태라고 가정한다. 이에 따라 단어 w는 다음 기준에 따라 topic이 재할당된다.\n",
    "    \n",
    "    p(topic | document d) : 문서 d의 단어들 중 topic t에 해당하는 단어들의 비율<br>\n",
    "    p(word w | topic t) : 단어 w를 갖고 있는 모든 문서들 중 topic t가 할당된 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased, disagree, st...\n",
       "1    [yeah, expect, people, read, actually, accept,...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4    [well, change, scoring, playoff, pool, unfortu...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 단어에 대한 정수 인코딩 수행 - (word_id, word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(tokenize_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenize_doc]\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faith\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64281"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA 알고리즘을 이용한 Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.036*\"space\" + 0.013*\"nasa\" + 0.008*\"launch\" + 0.008*\"earth\"')\n",
      "(1, '0.013*\"health\" + 0.010*\"medical\" + 0.007*\"rate\" + 0.007*\"pain\"')\n",
      "(2, '0.038*\"period\" + 0.021*\"power\" + 0.016*\"play\" + 0.015*\"scorer\"')\n",
      "(3, '0.018*\"windows\" + 0.015*\"thanks\" + 0.012*\"know\" + 0.012*\"anyone\"')\n",
      "(4, '0.019*\"would\" + 0.013*\"people\" + 0.011*\"think\" + 0.010*\"know\"')\n",
      "(5, '0.014*\"available\" + 0.011*\"information\" + 0.010*\"mail\" + 0.009*\"also\"')\n",
      "(6, '0.027*\"food\" + 0.010*\"diet\" + 0.008*\"prophecy\" + 0.007*\"chinese\"')\n",
      "(7, '0.036*\"window\" + 0.021*\"motif\" + 0.018*\"widget\" + 0.018*\"server\"')\n",
      "(8, '0.018*\"game\" + 0.017*\"team\" + 0.014*\"year\" + 0.013*\"games\"')\n",
      "(9, '0.009*\"government\" + 0.009*\"public\" + 0.008*\"system\" + 0.007*\"encryption\"')\n",
      "(10, '0.085*\"drive\" + 0.035*\"disk\" + 0.030*\"hard\" + 0.030*\"drives\"')\n",
      "(11, '0.037*\"file\" + 0.023*\"output\" + 0.019*\"entry\" + 0.017*\"program\"')\n",
      "(12, '0.049*\"israel\" + 0.031*\"israeli\" + 0.023*\"jews\" + 0.020*\"arab\"')\n",
      "(13, '0.024*\"jesus\" + 0.015*\"christian\" + 0.014*\"bible\" + 0.012*\"church\"')\n",
      "(14, '0.009*\"people\" + 0.008*\"said\" + 0.006*\"armenian\" + 0.006*\"armenians\"')\n",
      "(15, '0.007*\"cipher\" + 0.005*\"john\" + 0.005*\"pitcher\" + 0.004*\"missing\"')\n",
      "(16, '0.010*\"steve\" + 0.007*\"linked\" + 0.006*\"mark\" + 0.006*\"david\"')\n",
      "(17, '0.034*\"bike\" + 0.014*\"ride\" + 0.013*\"picture\" + 0.011*\"riding\"')\n",
      "(18, '0.009*\"like\" + 0.008*\"good\" + 0.007*\"used\" + 0.006*\"price\"')\n",
      "(19, '0.020*\"myers\" + 0.011*\"candida\" + 0.011*\"espn\" + 0.010*\"marriage\"')\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.036*\"space\" + 0.013*\"nasa\" + 0.008*\"launch\" + 0.008*\"earth\" + 0.007*\"satellite\" + 0.006*\"shuttle\" + 0.005*\"orbit\" + 0.005*\"station\" + 0.005*\"lunar\" + 0.005*\"mission\"'), (1, '0.013*\"health\" + 0.010*\"medical\" + 0.007*\"rate\" + 0.007*\"pain\" + 0.007*\"disease\" + 0.006*\"study\" + 0.006*\"among\" + 0.005*\"doctor\" + 0.005*\"research\" + 0.005*\"patients\"'), (2, '0.038*\"period\" + 0.021*\"power\" + 0.016*\"play\" + 0.015*\"scorer\" + 0.009*\"third\" + 0.009*\"second\" + 0.008*\"murphy\" + 0.008*\"brown\" + 0.007*\"first\" + 0.006*\"recchi\"'), (3, '0.018*\"windows\" + 0.015*\"thanks\" + 0.012*\"know\" + 0.012*\"anyone\" + 0.011*\"card\" + 0.011*\"would\" + 0.011*\"system\" + 0.010*\"please\" + 0.009*\"problem\" + 0.009*\"like\"'), (4, '0.019*\"would\" + 0.013*\"people\" + 0.011*\"think\" + 0.010*\"know\" + 0.010*\"like\" + 0.007*\"even\" + 0.007*\"time\" + 0.006*\"could\" + 0.006*\"well\" + 0.006*\"much\"'), (5, '0.014*\"available\" + 0.011*\"information\" + 0.010*\"mail\" + 0.009*\"also\" + 0.008*\"list\" + 0.008*\"software\" + 0.008*\"send\" + 0.007*\"version\" + 0.006*\"email\" + 0.006*\"data\"'), (6, '0.027*\"food\" + 0.010*\"diet\" + 0.008*\"prophecy\" + 0.007*\"chinese\" + 0.006*\"part\" + 0.006*\"pope\" + 0.006*\"foods\" + 0.005*\"reaction\" + 0.005*\"ingr\" + 0.005*\"rumors\"'), (7, '0.036*\"window\" + 0.021*\"motif\" + 0.018*\"widget\" + 0.018*\"server\" + 0.015*\"application\" + 0.014*\"display\" + 0.009*\"xterm\" + 0.009*\"export\" + 0.009*\"contrib\" + 0.009*\"using\"'), (8, '0.018*\"game\" + 0.017*\"team\" + 0.014*\"year\" + 0.013*\"games\" + 0.010*\"season\" + 0.009*\"play\" + 0.008*\"players\" + 0.008*\"last\" + 0.008*\"hockey\" + 0.008*\"league\"'), (9, '0.009*\"government\" + 0.009*\"public\" + 0.008*\"system\" + 0.007*\"encryption\" + 0.006*\"security\" + 0.006*\"chip\" + 0.006*\"used\" + 0.006*\"keys\" + 0.005*\"clipper\" + 0.005*\"information\"'), (10, '0.085*\"drive\" + 0.035*\"disk\" + 0.030*\"hard\" + 0.030*\"drives\" + 0.020*\"floppy\" + 0.019*\"controller\" + 0.013*\"simms\" + 0.012*\"bios\" + 0.010*\"motherboard\" + 0.008*\"nist\"'), (11, '0.037*\"file\" + 0.023*\"output\" + 0.019*\"entry\" + 0.017*\"program\" + 0.012*\"line\" + 0.009*\"build\" + 0.009*\"size\" + 0.008*\"input\" + 0.008*\"files\" + 0.008*\"section\"'), (12, '0.049*\"israel\" + 0.031*\"israeli\" + 0.023*\"jews\" + 0.020*\"arab\" + 0.019*\"water\" + 0.014*\"jewish\" + 0.011*\"tobacco\" + 0.011*\"palestinian\" + 0.007*\"land\" + 0.007*\"peace\"'), (13, '0.024*\"jesus\" + 0.015*\"christian\" + 0.014*\"bible\" + 0.012*\"church\" + 0.011*\"christians\" + 0.010*\"religion\" + 0.010*\"faith\" + 0.010*\"christ\" + 0.006*\"believe\" + 0.006*\"christianity\"'), (14, '0.009*\"people\" + 0.008*\"said\" + 0.006*\"armenian\" + 0.006*\"armenians\" + 0.005*\"government\" + 0.005*\"president\" + 0.005*\"turkish\" + 0.004*\"state\" + 0.004*\"states\" + 0.004*\"first\"'), (15, '0.007*\"cipher\" + 0.005*\"john\" + 0.005*\"pitcher\" + 0.004*\"missing\" + 0.004*\"grounded\" + 0.004*\"cryptanalysis\" + 0.004*\"patent\" + 0.004*\"paradox\" + 0.003*\"compress\" + 0.003*\"species\"'), (16, '0.010*\"steve\" + 0.007*\"linked\" + 0.006*\"mark\" + 0.006*\"david\" + 0.006*\"netcom\" + 0.005*\"francis\" + 0.005*\"dave\" + 0.005*\"smith\" + 0.005*\"andrew\" + 0.005*\"phillies\"'), (17, '0.034*\"bike\" + 0.014*\"ride\" + 0.013*\"picture\" + 0.011*\"riding\" + 0.011*\"motorcycle\" + 0.010*\"road\" + 0.010*\"bikes\" + 0.010*\"cubs\" + 0.009*\"helmet\" + 0.008*\"sleeve\"'), (18, '0.009*\"like\" + 0.008*\"good\" + 0.007*\"used\" + 0.006*\"price\" + 0.005*\"also\" + 0.005*\"would\" + 0.005*\"back\" + 0.005*\"much\" + 0.005*\"power\" + 0.005*\"time\"'), (19, '0.020*\"myers\" + 0.011*\"candida\" + 0.011*\"espn\" + 0.010*\"marriage\" + 0.009*\"easter\" + 0.007*\"syndrome\" + 0.006*\"surgery\" + 0.006*\"married\" + 0.005*\"yeast\" + 0.005*\"kong\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습\n",
    "\n",
    "https://serieson.naver.com/movie/recommendList.nhn\n",
    "\n",
    "위 site에서 영화별 줄거리를 crawling 수행 후 파일에 저장하고 DTM/TF-IDF, 문서 유사도, Topic modeling을 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
