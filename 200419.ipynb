{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Open\n",
    "1. Review\n",
    "2. Topic\n",
    "\n",
    "    2.1 태깅 작업\n",
    "    \n",
    "    2.2 seq2seq\n",
    "    \n",
    "3. Q & A\n",
    "4. Next\n",
    "5. Close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 이해 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅된 문장 개수 : 3914\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "print('품사 태깅된 문장 개수 : {}'.format(len(tagged_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, pos_tags = [], []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tag_info = zip(*tagged_sentence)\n",
    "    sentences.append(list(sentence))\n",
    "    pos_tags.append(list(tag_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(pos_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 271\n",
      "샘플의 평균 길이 : 25.722023505365357\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZ30lEQVR4nO3df9xmdV3n8ddbBDTDRmRgZ/nhQM7DtC1xHI1dyUXZTMBtcFdQ2mJEanaLUjNbh7WSetSGW/mzFkUxBzON1QhWySSCyFXQASZAWWLEUSZYZlTkZ2jgZ/843/vi6p77nvvMj+u65r7v1/PxOI9zzvc651yfL2e4P9f3e875nlQVkiQBPG7SAUiS9h4mBUnSgElBkjRgUpAkDZgUJEkDJgVJ0sDIkkKSZyTZODTdl+T1SQ5McnmS29r8KW37JHlXkk1JbkyyclSxSZJmNrKkUFW3VtXRVXU08FzgIeBiYB1wRVWtAK5o6wAnACvatBY4b1SxSZJmNq7uo+OBL1fVV4HVwPpWvh44uS2vBi6szjXAkiTLxhSfJAl4/Ji+51XAR9ryIVV1F0BV3ZXk4FZ+KHDH0D5bWtldsx30oIMOquXLl+/5aCVpAbvuuuu+XlVLZ/ps5EkhyX7ATwBnz7XpDGXbjcGRZC1d9xJHHHEEGzZs2O0YJWkxSfLV2T4bR/fRCcD1VXV3W797qluozbe28i3A4UP7HQbcOf1gVXV+Va2qqlVLl86Y6CRJu2gcSeE0Hus6ArgUWNOW1wCXDJWf3u5COga4d6qbSZI0HiPtPkryPcCPAf95qPhc4KIkZwJfA05p5ZcBJwKb6O5UOmOUsUmStjfSpFBVDwFPnVb2Dbq7kaZvW8BZo4xHkrRjPtEsSRowKUiSBkwKkqQBk4IkacCkIEkaGNcwF4vS8nWfnLF887knjTkSSerHloIkacCkIEkaMClIkgZMCpKkAS807wGzXVCWpPnGloIkacCkIEkaMClIkgZMCpKkAZOCJGnApCBJGjApSJIGTAqSpAEfXpsAR0+VtLeypSBJGjApSJIGRtp9lGQJ8H7gXwEFvAa4FfhTYDmwGTi1qu5JEuCdwInAQ8Crq+r6Uca3MxzfSNJiMOqWwjuBT1XVDwDPBm4B1gFXVNUK4Iq2DnACsKJNa4HzRhybJGmakSWFJE8GXghcAFBV36mqbwGrgfVts/XAyW15NXBhda4BliRZNqr4JEnbG2VL4ShgG/BHSW5I8v4kTwIOqaq7ANr84Lb9ocAdQ/tvaWWSpDEZZVJ4PLASOK+qngM8yGNdRTPJDGW13UbJ2iQbkmzYtm3bnolUkgSMNilsAbZU1bVt/WN0SeLuqW6hNt86tP3hQ/sfBtw5/aBVdX5VraqqVUuXLh1Z8JK0GI0sKVTV/wPuSPKMVnQ88CXgUmBNK1sDXNKWLwVOT+cY4N6pbiZJ0niM+onmXwQ+nGQ/4HbgDLpEdFGSM4GvAae0bS+jux11E90tqWeMODZJ0jQjTQpVtRFYNcNHx8+wbQFnjTIeSdKO+USzJGnApCBJGjApSJIGTAqSpAGTgiRpwKQgSRowKUiSBkwKkqQBk4IkacCkIEkaMClIkgZMCpKkAZOCJGnApCBJGjApSJIGTAqSpAGTgiRpwKQgSRowKUiSBkwKkqQBk4IkacCkIEkaGGlSSLI5yU1JNibZ0MoOTHJ5ktva/CmtPEnelWRTkhuTrBxlbJKk7Y2jpfCiqjq6qla19XXAFVW1AriirQOcAKxo01rgvDHEJkkaMonuo9XA+ra8Hjh5qPzC6lwDLEmybALxSdKiNWdSSHJKkgPa8q8m+bOd6Nop4NNJrkuytpUdUlV3AbT5wa38UOCOoX23tDJJ0pj0aSn8WlXdn+RY4Mfpft337dp5QVWtpOsaOivJC3ewbWYoq+02StYm2ZBkw7Zt23qGIUnqo09SeLTNTwLOq6pLgP36HLyq7mzzrcDFwPOBu6e6hdp8a9t8C3D40O6HAXfOcMzzq2pVVa1aunRpnzAkST31SQr/kOS9wKnAZUn277NfkicNdTs9CXgJcDNwKbCmbbYGuKQtXwqc3u5COga4d6qbSZI0Ho/vsc2pwEuB36uqb7Vf97/SY79DgIuTTH3Pn1TVp5J8AbgoyZnA14BT2vaXAScCm4CHgDN2qiaSpN02Z1KoqoeSbAWOBW4DHmnzufa7HXj2DOXfAI6fobyAs3rELEkakT7dQG8B3gSc3Yr2Bf54lEFJkiajzzWFlwM/ATwIg4vHB4wyKEnSZPRJCt9pXTsFg4vGkqQFqE9SuKjdfbQkyc8CfwW8b7RhSZImoc+F5t9L8mPAfcAzgF+vqstHHpkkaez63JJKSwImAkla4GZNCknuZ4ZhJuiGo6iqevLIopIkTcSsSaGqvMNIkhaZXt1HbVTUY+laDp+pqhtGGpUkaSL6PLz263Qjoz4VOAj4YJJfHXVgkqTx69NSOA14TlU9DJDkXOB64LdGGZgkafz6PKewGXjC0Pr+wJdHEo0kaaL6tBS+DXwxyeV01xR+DPhMkncBVNVrRxifJGmM+iSFi9s05arRhCJJmrQ+TzSvH0cgkqTJ63P30cuS3JDkm0nuS3J/kvvGEZwkabz6dB+9A/gPwE1ttFRJ0gLV5+6jO4CbTQiStPD1aSn8V+CyJH9DdycSAFX1tpFFJUmaiD5J4beBB+ieVdhvtOFIkiapT1I4sKpeMvJIJEkT1+eawl8lMSlI0iLQJymcBXwqyT/uyi2pSfZpt7R+oq0fmeTaJLcl+dMk+7Xy/dv6pvb58l2pkCRp182ZFKrqgKp6XFU9saqe3NZ35gU7rwNuGVp/K/D2qloB3AOc2crPBO6pqqcDb2/bSZLGqE9LgSRPSfL8JC+cmnrudxhwEvD+th7gxcDH2ibrgZPb8uq2Tvv8+La9JGlM5rzQnORn6H7tHwZsBI4BPkf3x30u76C7pXXqLW5PBb5VVY+09S3AoW35ULpnIqiqR5Lc27b/eq+aSJJ2W5+WwuuA5wFfraoXAc8Bts21U5KXAVur6rrh4hk2rR6fDR93bZINSTZs2zZnGJKkndAnKTw89IKd/avq/wLP6LHfC4CfSLIZ+Chdy+IdwJIkUy2Uw4A72/IW4PD2PY8Hvg/45vSDVtX5VbWqqlYtXbq0RxiSpL76PKewJckS4M+By5Pcw2N/yGdVVWcDZwMkOQ54Y1X9pyT/C3gFXaJYA1zSdrm0rX+uff7Xi21ojeXrPjlj+eZzTxpzJJIWqz5DZ7+8LZ6T5Eq6X/Cf2o3vfBPw0SS/BdwAXNDKLwA+lGQTXQvhVbvxHZKkXdDnQvP3A1uq6tt0/f7Lge8BvtP3S6rqKtrLearqduD5M2zzMHBK32NKkva8PtcUPg48muTpdL/mjwT+ZKRRSZImok9S+G67hfTlwDuq6peAZaMNS5I0CX2Swj8lOY3uIvAnWtm+owtJkjQpfZLCGcC/Bn67qr6S5Ejgj0cbliRpEvrcffQl4LVD618Bzh1lUJKkyeg19pEkaXEwKUiSBmZNCkk+1OavG184kqRJ2lFL4blJnga8pg2dfeDwNK4AJUnjs6MLze+hG87iKOA6/vkoptXKJUkLyKwthap6V1U9E/hAVR1VVUcOTSYESVqA+tyS+nNJng38aCu6uqpuHG1YkqRJmPPuoySvBT4MHNymDyf5xVEHJkkavz7vU/gZ4Eeq6kGAJG+le+fBu0cZmCRp/Po8pxDg0aH1R5n51ZmSpHmuT0vhj4Brk1zc1k/msRfjSJIWkD4Xmt+W5CrgWLoWwhlVdcOoA5MkjV+flgJVdT1w/YhjkSRNmGMfSZIGTAqSpIEdJoUk+yT5q3EFI0marB0mhap6FHgoyfeNKR5J0gT1udD8MHBTksuBB6cKq+q1s+8CSZ4AXA3s377nY1X1lvY6z48CB9JdvP7pqvpOkv2BC4HnAt8AXllVm3e+SpKkXdUnKXyyTTvr28CLq+qBJPsCn0nyF8AbgLdX1UeTvAc4Ezivze+pqqcneRXwVuCVu/C9kqRd1Oc5hfVJnggcUVW39j1wVRXwQFvdt00FvBj4yVa+HjiHLimsbssAHwP+IEnacSRJY9BnQLx/D2yke7cCSY5Ocmmfg7cL1RuBrcDlwJeBb1XVI22TLcChbflQ4A6A9vm9wFP7V0WStLv6dB+dAzwfuAqgqja26wJzaheqj06yBLgYeOZMm7X5TOMpbddKSLIWWAtwxBFH9Aljpyxftys9ZZK0MPR5TuGRqrp3WtlOdelU1bfoksoxwJIkU8noMODOtrwFOBygff59wDdnONb5VbWqqlYtXbp0Z8KQJM2hT1K4OclPAvskWZHk3cBn59opydLWQqBdk/h3wC3AlcAr2mZrgEva8qVtnfb5X3s9QZLGq09S+EXgB+nuJvoIcB/w+h77LQOuTHIj8AXg8qr6BPAm4A1JNtFdM5gacfUC4Kmt/A3Aup2piCRp9/W5++gh4M3t5TpVVff3OXB7ZedzZii/ne4axfTyh4FT+hxbkjQafe4+el6Sm4Ab6R5i+7skzx19aJKkcetz99EFwM9X1d8CJDmW7sU7PzzKwPSY2e6I2nzuSWOORNJC1+eawv1TCQGgqj4D9OpCkiTNL7O2FJKsbIufT/JeuovMRTf0xFWjD02SNG476j76/Wnrbxla9lZRSVqAZk0KVfWicQYiSZq8OS80twfQTgeWD28/19DZkqT5p8/dR5cB1wA3Ad8dbTiSpEnqkxSeUFVvGHkkkqSJ63NL6oeS/GySZUkOnJpGHpkkaez6tBS+A/wu8GYeu+uogKNGFZQkaTL6JIU3AE+vqq+POhhJ0mT16T76IvDQqAORJE1en5bCo8DGJFfSDZ8NeEuqJC1EfZLCn7dJkrTA9XmfwvpxBCJJmrw+TzR/hRnGOqoq7z6SpAWmT/fRqqHlJ9C9Hc3nFCRpAZrz7qOq+sbQ9A9V9Q7gxWOITZI0Zn26j1YOrT6OruVwwMgikiRNTJ/uo+H3KjwCbAZOHUk0kqSJ6nP3ke9VkKRFok/30f7Af2T79yn85hz7HQ5cCPwLuiG3z6+qd7bB9P60HW8zcGpV3ZMkwDuBE+meoH51VV2/81WSJO2qPsNcXAKspus6enBomssjwC9X1TOBY4CzkjwLWAdcUVUrgCvaOsAJwIo2rQXO24l6SJL2gD7XFA6rqpfu7IGr6i7grrZ8f5JbgEPpEsxxbbP1wFXAm1r5hVVVwDVJliRZ1o4jSRqDPi2Fzyb5od35kiTLgecA1wKHTP2hb/OD22aHAncM7ballUmSxqRPS+FY4NXtyeZvAwGqqn64zxck+V7g48Drq+q+7tLBzJvOULbdk9RJ1tJ1L3HEEUf0CUGS1FOfpHDCrh48yb50CeHDVfVnrfjuqW6hJMuAra18C3D40O6HAXdOP2ZVnQ+cD7Bq1artkoYkadf1eaL5qzNNc+3X7ia6ALilqt429NGlwJq2vIbuQvZU+enpHAPc6/UESRqvPi2FXfUC4KeBm5JsbGX/DTgXuCjJmcDX6MZSAriM7nbUTXS3pJ4xwtgkSTMYWVKoqs8w83UCgONn2L6As0YVjyRpbn3uPpIkLRImBUnSgElBkjRgUpAkDZgUJEkDJgVJ0oBJQZI0YFKQJA2YFCRJAyYFSdKASUGSNGBSkCQNmBQkSQMmBUnSgElBkjRgUpAkDZgUJEkDJgVJ0oBJQZI0MLJ3NO/tlq/75KRD2G2z1WHzuSeNORJJC4UtBUnSgElBkjQwsqSQ5ANJtia5eajswCSXJ7mtzZ/SypPkXUk2JbkxycpRxSVJmt0oWwofBF46rWwdcEVVrQCuaOsAJwAr2rQWOG+EcUmSZjGypFBVVwPfnFa8GljfltcDJw+VX1ida4AlSZaNKjZJ0szGfU3hkKq6C6DND27lhwJ3DG23pZVJksZob7nQnBnKasYNk7VJNiTZsG3bthGHJUmLy7iTwt1T3UJtvrWVbwEOH9ruMODOmQ5QVedX1aqqWrV06dKRBitJi824k8KlwJq2vAa4ZKj89HYX0jHAvVPdTJKk8RnZE81JPgIcBxyUZAvwFuBc4KIkZwJfA05pm18GnAhsAh4CzhhVXJKk2Y0sKVTVabN8dPwM2xZw1qhikST1s7dcaJYk7QVMCpKkAZOCJGnApCBJGjApSJIGTAqSpIFF++a1hcw3sknaVbYUJEkDJgVJ0oBJQZI0YFKQJA2YFCRJAyYFSdKAt6QuIt6qKmkuthQkSQMmBUnSgElBkjRgUpAkDZgUJEkDJgVJ0oBJQZI0YFKQJA3sVQ+vJXkp8E5gH+D9VXXuhENaFHyoTdKUvaalkGQf4A+BE4BnAacledZko5KkxWVvaik8H9hUVbcDJPkosBr40kSjWsRsQUiLz96UFA4F7hha3wL8yIRi0Q7Mliz2pD2VeHY2se1s3UyQGrVx/zjbm5JCZiir7TZK1gJr2+oDSW7dhe86CPj6Luw3n8zrOuatc26yW/XrcfyxHmcG8/r89bTQ6zjS+u3mv72nzfbB3pQUtgCHD60fBtw5faOqOh84f3e+KMmGqlq1O8fY2y30Olq/+W+h13G+1m+vudAMfAFYkeTIJPsBrwIunXBMkrSo7DUthap6JMkvAH9Jd0vqB6rqixMOS5IWlb0mKQBU1WXAZWP4qt3qfponFnodrd/8t9DrOC/rl6rtruVKkhapvemagiRpwhZdUkjy0iS3JtmUZN2k49kTkmxOclOSjUk2tLIDk1ye5LY2f8qk49wZST6QZGuSm4fKZqxTOu9q5/TGJCsnF3k/s9TvnCT/0M7jxiQnDn12dqvfrUl+fDJR95fk8CRXJrklyReTvK6VL4hzuIP6zf9zWFWLZqK7gP1l4ChgP+DvgGdNOq49UK/NwEHTyv4HsK4trwPeOuk4d7JOLwRWAjfPVSfgROAv6J51OQa4dtLx72L9zgHeOMO2z2r/VvcHjmz/hveZdB3mqN8yYGVbPgD4+1aPBXEOd1C/eX8OF1tLYTCURlV9B5gaSmMhWg2sb8vrgZMnGMtOq6qrgW9OK56tTquBC6tzDbAkybLxRLprZqnfbFYDH62qb1fVV4BNdP+W91pVdVdVXd+W7wduoRu1YEGcwx3Ubzbz5hwutqQw01AaOzqR80UBn05yXXviG+CQqroLun/AwMETi27Pma1OC+m8/kLrPvnAUJffvK5fkuXAc4BrWYDncFr9YJ6fw8WWFHoNpTEPvaCqVtKNMHtWkhdOOqAxWyjn9Tzg+4GjgbuA32/l87Z+Sb4X+Djw+qq6b0ebzlC219dxhvrN+3O42JJCr6E05puqurPNtwIX0zVL755qfrf51slFuMfMVqcFcV6r6u6qerSqvgu8j8e6F+Zl/ZLsS/cH88NV9WeteMGcw5nqtxDO4WJLCgtuKI0kT0pywNQy8BLgZrp6rWmbrQEumUyEe9RsdboUOL3dwXIMcO9UF8V8Mq0P/eV05xG6+r0qyf5JjgRWAJ8fd3w7I0mAC4BbquptQx8tiHM4W/0WxDmc9JXucU90dzn8Pd3V/zdPOp49UJ+j6O5q+Dvgi1N1Ap4KXAHc1uYHTjrWnazXR+ia3/9E9yvrzNnqRNc0/8N2Tm8CVk06/l2s34da/DfS/RFZNrT9m1v9bgVOmHT8Pep3LF33yI3AxjaduFDO4Q7qN+/PoU80S5IGFlv3kSRpB0wKkqQBk4IkacCkIEkaMClIkgZMCpo3kjwwgmMePW0ky3OSvHE3jndKGznzyj0T4S7HsTnJQZOMQfOTSUGL3dF095fvKWcCP19VL9qDx5TGxqSgeSnJryT5Qht47Dda2fL2K/19bYz7Tyd5YvvseW3bzyX53SQ3t6fafxN4ZRv7/pXt8M9KclWS25O8dpbvPy3dOyxuTvLWVvbrdA81vSfJ707bflmSq9v33JzkR1v5eUk2tHh/Y2j7zUn+e4t3Q5KVSf4yyZeT/Je2zXHtmBcn+VKS9yTZ7v/pJD+V5PPtu9+bZJ82fbDFclOSX9rNU6KFYtJPzzk59Z2AB9r8JXTvvw3dD5tP0L2fYDnwCHB02+4i4Kfa8s3Av2nL59LeYwC8GviDoe84B/gs3bj3BwHfAPadFse/BL4GLKV7z/lfAye3z65ihqdxgV/msafN9wEOaMsHDpVdBfxwW98M/FxbfjvdE7IHtO/c2sqPAx6me6p9H+By4BVD+x8EPBP431N1AP4ncDrwXODyofiWTPr8Ou0dky0FzUcvadMNwPXAD9CNJQPwlara2JavA5YnWUL3R/izrfxP5jj+J6sb9/7rdAO2HTLt8+cBV1XVtqp6BPgwXVLakS8AZyQ5B/ih6sbgBzg1yfWtLj9I9zKWKVPjct1E99KZ+6tqG/BwqxPA56t7P8ijdENnHDvte4+nSwBfSLKxrR8F3A4cleTdSV4K7GgEUy0ij590ANIuCPA7VfXef1bYjWv/7aGiR4EnMvOwxTsy/RjT/z/Z2eNRVVe3Ic1PAj7Uupf+Fngj8LyquifJB4EnzBDHd6fF9N2hmKaPUzN9PcD6qjp7ekxJng38OHAWcCrwmp2tlxYeWwqaj/4SeE0by54khyaZ9SVCVXUPcH8bfRO60XGn3E/XLbMzrgX+bZKDkuwDnAb8zY52SPI0um6f99GNrrkSeDLwIHBvkkPo3oexs57fRv19HPBK4DPTPr8CeMXUf59070h+Wrsz6XFV9XHg11o8ki0FzT9V9ekkzwQ+141gzAPAT9H9qp/NmcD7kjxI13d/byu/EljXulZ+p+f335Xk7LZvgMuqaq6hyY8DfiXJP7V4T6+qryS5gW5029uB/9Pn+6f5HN01kh8CrqZ7n8ZwrF9K8qt0b+Z7HN2orGcB/wj80dCF6e1aElqcHCVVi0KS762qB9ryOrohjV834bB2S5Lj6F4S/7JJx6KFw5aCFouT2q/7xwNfpbvrSNI0thQkSQNeaJYkDZgUJEkDJgVJ0oBJQZI0YFKQJA2YFCRJA/8fCoMq23LCuucAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('샘플의 최대 길이 : {}'.format(max(len(l) for l in sentences)))\n",
    "print('샘플의 평균 길이 : {}'.format(sum(map(len, sentences))/len(sentences)))\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(samples)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = tokenize(sentences)\n",
    "ter_tokenizer = tokenize(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 크기 : 11388\n",
      "태깅 정보 집합 크기 : 47\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(src_tokenizer.word_index) + 1\n",
    "tag_size = len(ter_tokenizer.word_index) + 1\n",
    "print('단어 집합 크기 : {}'.format(vocab_size))\n",
    "print('태깅 정보 집합 크기 : {}'.format(tag_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = ter_tokenizer.texts_to_sequences(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5601, 3746, 1, 2024, 86, 331, 1, 46, 2405, 2, 131, 27, 6, 2025, 332, 459, 2026, 3], [31, 3746, 20, 177, 4, 5602, 2915, 1, 2, 2916, 637, 147, 3]]\n",
      "[[3, 3, 8, 10, 6, 7, 8, 21, 13, 4, 1, 2, 4, 7, 1, 3, 10, 9], [3, 3, 17, 1, 2, 3, 3, 8, 4, 3, 19, 1, 9]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:2])\n",
    "print(y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련/테스트 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = tag_size)\n",
    "y_test = to_categorical(y_test, num_classes = tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 문장 크기 : (3131, 150)\n",
      "훈련 레이블 문장 크기 : (3131, 150, 47)\n",
      "테스트 샘플 문장 크기 : (783, 150)\n",
      "테스트 레이블 문장 크기 : (783, 150, 47)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 샘플 문장 크기 : {}'.format(X_train.shape))\n",
    "print('훈련 레이블 문장 크기 : {}'.format(y_train.shape))\n",
    "print('테스트 샘플 문장 크기 : {}'.format(X_test.shape))\n",
    "print('테스트 레이블 문장 크기 : {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 양방향 LSTM 이용 POS Tagger 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/6\n",
      " 128/3131 [>.............................] - ETA: 5:11 - loss: 0.6809 - accuracy: 0.0106"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=6, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('테스트 정확도 : %.4f' %(model.evaluate(X_test, y_test[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = src_tokenizer.index_word\n",
    "index_to_tag = ter_tokenizer.index_word\n",
    "\n",
    "i = 10\n",
    "y_predicted = model.predict(np.array(X_test[i]))\n",
    "y_predicted = np.argmax(y_predicted, axis=-1)\n",
    "true = np.argmax(y_test[i], -1)\n",
    "\n",
    "print('{:15}|{:15}|{}'.format('단어', '실제값', '예측값'))\n",
    "print(35*'-')\n",
    "for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n",
    "    if w != 0:\n",
    "        print('{:17}:{:7} {}'.format(index_to_word[w], index_to_tag[t].upper(), index_to_tag[pred].upper()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 양방향 LSTM과 CRF(Bidirectional LSTM + CRF) 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF(Conditional Random Field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CRF는 양방향 LSTM을 위한 모델은 아니고 독자적으로 존재하는 모델이다.\n",
    "- CRF를 양방향 LSTM 모델 위에 하나의 층으로 추가하여, 양방향 LSTM + CRF 모델을 구축하여 기존 양방향 LSTM을 이용한 태깅 작업에 대한 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 양방향 LSTM 위에 CRF 층을 추가하여 얻을 수 있는 이점은 레이블 사이의 의존성을 고려할 수 있다는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 문장의 첫번째 단어에서는 I가 나오지 않는다.\n",
    "2, O-I 패턴은 나오지 않는다.\n",
    "3. B-I-I에서 개체명은 일관성을 유지한다. 예로 B-per 다음에 O-org는 나오지 않는다.\n",
    "\n",
    "- 양방향 LSTM은 입력 단어에 대한 양방향 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF layer는 tensorflow 1.3, keras 2.2.4에서만 동작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시퀀스 레이블링 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시퀀스 레이블링 모델을 평가할 때 주의할 점은 큰 의미를 갖지 않는 레이블 정보가 존재하고 이로 인해 정확도가 높은 평가에 방해가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "true = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O',\n",
    "        'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O',\n",
    "        'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O',\n",
    "        'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
    "predicted = ['O'] * len(true)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 74.4\n"
     ]
    }
   ],
   "source": [
    "hit = 0\n",
    "\n",
    "for t, p in zip(true, predicted):\n",
    "    if t == p:\n",
    "        hit += 1\n",
    "accuracy = hit / len(true) * 100\n",
    "print('정확도 : %.1f' %(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 개체명 인식 모델의 성능 측정을 위한 정밀도와 재현율 개념\n",
    "\n",
    "- 정밀도 = TP / TP + FP : 특정 객체라고 예측한 경우 중 실제 특정 개체로 판명되어 예측이 일치하는 비율\n",
    "\n",
    "- 재현율 = TP / TF + FN : 전체 특징 개체 중에서 실제 특정 개체라고 정답을 맞춘 비율\n",
    "\n",
    "#### 정밀도의 재현율로부터 조화 평균(harmonic mean)을 구한 것을 f1-score라 함\n",
    "\n",
    "- f1 score = 2 X 정밀도 X 재현률 / 정밀도 + 재현율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-0.0.12.tar.gz (21 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (from seqeval) (1.18.1)\n",
      "Collecting Keras>=2.2.4\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (from Keras>=2.2.4->seqeval) (1.14.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (from Keras>=2.2.4->seqeval) (5.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7429 sha256=09d13f93019a69586546658161fabd464b6687f683f50029117933aee5646b49\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\dc\\cc\\62\\a3b81f92d35a80e39eb9b2a9d8b31abac54c02b21b2d466edc\n",
      "Successfully built seqeval\n",
      "Installing collected packages: Keras, seqeval\n",
      "Successfully installed Keras-2.3.1 seqeval-0.0.12\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      PER       0.00      0.00      0.00         3\n",
      "     MISC       0.00      0.00      0.00         2\n",
      "\n",
      "micro avg       0.00      0.00      0.00         5\n",
      "macro avg       0.00      0.00      0.00         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "true = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O',\n",
    "        'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O',\n",
    "        'O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O',\n",
    "        'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
    "predicted = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O',\n",
    "             'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
    "             'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
    "             'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      PER       1.00      0.67      0.80         3\n",
      "     MISC       1.00      0.50      0.67         2\n",
      "\n",
      "micro avg       1.00      0.60      0.75         5\n",
      "macro avg       1.00      0.60      0.75         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence(seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq(Sequence-to-Sequence) : 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델\n",
    "\n",
    "- seq2seq 사용 분야\n",
    "    1. 챗봇(Chatbot) : 입력 시퀀스는 질문, 출력 시퀀스는 답변으로 구성\n",
    "    2. 기계 번역(Machine Translator) : 입력 시퀀스는 입력 문장, 출력 시퀀스는 번역 문장으로 구성\n",
    "    3. 내용 요약(Text Summarization) : 입력 시퀀스는 원본 문서, 출력 시퀀스는 요약 내용으로 구성\n",
    "    4. STT(Speech to Text) : 입력 시퀀스는 음성 데이터, 출력 시퀀스는 음성에 대한 자연어로 구성\n",
    "    \n",
    "- seq2seq는 RNN을 기반으로 한 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Counter Vector : 인코더의 처리 결과(은닉 상태)를 디코더의 입력(은닉 상태)으로 전달되는 vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175623"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv('fra-eng/fra.txt', names=['src', 'tar', 'comment'], sep='\\t')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46718</th>\n",
       "      <td>Have you been drinking?</td>\n",
       "      <td>As-tu bu ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>Feel this.</td>\n",
       "      <td>Sens ça !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>He can swim.</td>\n",
       "      <td>Il sait nager.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22097</th>\n",
       "      <td>You can't give up.</td>\n",
       "      <td>Vous ne pouvez pas arrêter.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34541</th>\n",
       "      <td>How do you want them?</td>\n",
       "      <td>Comment les veux-tu ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45445</th>\n",
       "      <td>You don't have to lie.</td>\n",
       "      <td>Vous n'êtes pas obligées de mentir.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>He tries.</td>\n",
       "      <td>Il essaye.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36144</th>\n",
       "      <td>I'm really busy, Tom.</td>\n",
       "      <td>Je suis très occupé, Tom.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37194</th>\n",
       "      <td>Speak slowly, please.</td>\n",
       "      <td>Parlez lentement, je vous prie.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55840</th>\n",
       "      <td>It's awfully cold today.</td>\n",
       "      <td>Aujourd’hui, il fait horriblement froid.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                       tar  \\\n",
       "46718   Have you been drinking?                                As-tu bu ?   \n",
       "541                  Feel this.                                 Sens ça !   \n",
       "2054               He can swim.                            Il sait nager.   \n",
       "22097        You can't give up.               Vous ne pouvez pas arrêter.   \n",
       "34541     How do you want them?                     Comment les veux-tu ?   \n",
       "45445    You don't have to lie.       Vous n'êtes pas obligées de mentir.   \n",
       "284                   He tries.                                Il essaye.   \n",
       "36144     I'm really busy, Tom.                 Je suis très occupé, Tom.   \n",
       "37194     Speak slowly, please.           Parlez lentement, je vous prie.   \n",
       "55840  It's awfully cold today.  Aujourd’hui, il fait horriblement froid.   \n",
       "\n",
       "                                                 comment  \n",
       "46718  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "541    CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "2054   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "22097  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "34541  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "45445  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "284    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "36144  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "37194  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "55840  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[0:60000]\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>You're nuts!</td>\n",
       "      <td>\\tVous êtes givrée !\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57785</th>\n",
       "      <td>Try to enjoy yourselves.</td>\n",
       "      <td>\\tEssayez d'en profiter.\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59424</th>\n",
       "      <td>Children filled the room.</td>\n",
       "      <td>\\tLes enfants ont rempli la pièce.\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56263</th>\n",
       "      <td>Please close the window.</td>\n",
       "      <td>\\tVeuillez fermer la fenêtre.\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19230</th>\n",
       "      <td>I need a keyboard.</td>\n",
       "      <td>\\tIl me faut un clavier.\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38393</th>\n",
       "      <td>We were all so happy.</td>\n",
       "      <td>\\tNous étions toutes si heureuses.\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6954</th>\n",
       "      <td>You are early.</td>\n",
       "      <td>\\tVous êtes matinale.\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9846</th>\n",
       "      <td>You're winning.</td>\n",
       "      <td>\\tVous l'emportez.\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58316</th>\n",
       "      <td>Where are your car keys?</td>\n",
       "      <td>\\tOù sont tes clés de voiture ?\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19385</th>\n",
       "      <td>I still have mine.</td>\n",
       "      <td>\\tJ'ai toujours la mienne.\\n</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             src                                   tar  \\\n",
       "3129                You're nuts!                \\tVous êtes givrée !\\n   \n",
       "57785   Try to enjoy yourselves.            \\tEssayez d'en profiter.\\n   \n",
       "59424  Children filled the room.  \\tLes enfants ont rempli la pièce.\\n   \n",
       "56263   Please close the window.       \\tVeuillez fermer la fenêtre.\\n   \n",
       "19230         I need a keyboard.            \\tIl me faut un clavier.\\n   \n",
       "38393      We were all so happy.  \\tNous étions toutes si heureuses.\\n   \n",
       "6954              You are early.               \\tVous êtes matinale.\\n   \n",
       "9846             You're winning.                  \\tVous l'emportez.\\n   \n",
       "58316   Where are your car keys?     \\tOù sont tes clés de voiture ?\\n   \n",
       "19385         I still have mine.          \\tJ'ai toujours la mienne.\\n   \n",
       "\n",
       "                                                 comment  \n",
       "3129   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "57785  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "59424  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "56263  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "19230  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "38393  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "6954   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "9846   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "58316  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "19385  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t' + x + '\\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "        \n",
    "tar_vocab = set()\n",
    "\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab) + 1\n",
    "tar_vocab_size = len(tar_vocab) + 1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "# sort for indexing\n",
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, 'С': 100, '\\u2009': 101, '\\u200b': 102, '‘': 103, '’': 104, '\\u202f': 105}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(src_to_index[w])\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 48, 53, 3, 4, 2], [1, 45, 53, 64, 73, 72, 3, 4, 2], [1, 45, 53, 64, 73, 72, 14, 2], [1, 29, 67, 73, 70, 71, 105, 4, 2], [1, 29, 67, 73, 70, 57, 78, 105, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48, 53, 3, 4, 2], [45, 53, 64, 73, 72, 3, 4, 2], [45, 53, 64, 73, 72, 14, 2], [29, 67, 73, 70, 71, 105, 4, 2], [29, 67, 73, 70, 57, 78, 105, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t = 0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t > 0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t += 1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련을 위한 padding과 one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq 기계 번역 모델 구축 / 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 375s 8ms/sample - loss: 0.7828 - val_loss: 0.7048\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 361s 8ms/sample - loss: 0.4889 - val_loss: 0.5642\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 362s 8ms/sample - loss: 0.4057 - val_loss: 0.4959\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 370s 8ms/sample - loss: 0.3596 - val_loss: 0.4590\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 354s 7ms/sample - loss: 0.3294 - val_loss: 0.4324\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 357s 7ms/sample - loss: 0.3076 - val_loss: 0.4139\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 358s 7ms/sample - loss: 0.2908 - val_loss: 0.4031\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 350s 7ms/sample - loss: 0.2772 - val_loss: 0.3917\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 357s 7ms/sample - loss: 0.2658 - val_loss: 0.3871\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 390s 8ms/sample - loss: 0.2561 - val_loss: 0.3816\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 383s 8ms/sample - loss: 0.2475 - val_loss: 0.3761\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 373s 8ms/sample - loss: 0.2400 - val_loss: 0.3741\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 322s 7ms/sample - loss: 0.2331 - val_loss: 0.3731\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 319s 7ms/sample - loss: 0.2270 - val_loss: 0.3732\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 315s 7ms/sample - loss: 0.2213 - val_loss: 0.3712\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 319s 7ms/sample - loss: 0.2161 - val_loss: 0.3706\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 320s 7ms/sample - loss: 0.2112 - val_loss: 0.3715\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 318s 7ms/sample - loss: 0.2066 - val_loss: 0.3717\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 318s 7ms/sample - loss: 0.2024 - val_loss: 0.3744\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 314s 7ms/sample - loss: 0.1982 - val_loss: 0.3745\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 315s 7ms/sample - loss: 0.1945 - val_loss: 0.3768\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 317s 7ms/sample - loss: 0.1909 - val_loss: 0.3767\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 317s 7ms/sample - loss: 0.1876 - val_loss: 0.3807\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 317s 7ms/sample - loss: 0.1843 - val_loss: 0.3817\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 317s 7ms/sample - loss: 0.1813 - val_loss: 0.3832\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 315s 7ms/sample - loss: 0.1782 - val_loss: 0.3834\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 317s 7ms/sample - loss: 0.1753 - val_loss: 0.3865\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 352s 7ms/sample - loss: 0.1727 - val_loss: 0.3899\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 368s 8ms/sample - loss: 0.1700 - val_loss: 0.3935\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 356s 7ms/sample - loss: 0.1674 - val_loss: 0.3943\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 334s 7ms/sample - loss: 0.1652 - val_loss: 0.3961\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 335s 7ms/sample - loss: 0.1629 - val_loss: 0.4000\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 330s 7ms/sample - loss: 0.1606 - val_loss: 0.4033\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 331s 7ms/sample - loss: 0.1585 - val_loss: 0.4050\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 331s 7ms/sample - loss: 0.1565 - val_loss: 0.4096\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 330s 7ms/sample - loss: 0.1546 - val_loss: 0.4111\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 352s 7ms/sample - loss: 0.1526 - val_loss: 0.4138\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 330s 7ms/sample - loss: 0.1507 - val_loss: 0.4157\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 327s 7ms/sample - loss: 0.1489 - val_loss: 0.4202\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 333s 7ms/sample - loss: 0.1472 - val_loss: 0.4236\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 321s 7ms/sample - loss: 0.1456 - val_loss: 0.4248\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 318s 7ms/sample - loss: 0.1440 - val_loss: 0.4283\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 320s 7ms/sample - loss: 0.1425 - val_loss: 0.4315\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 319s 7ms/sample - loss: 0.1409 - val_loss: 0.4317\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 329s 7ms/sample - loss: 0.1395 - val_loss: 0.4346\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 332s 7ms/sample - loss: 0.1379 - val_loss: 0.4388\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 345s 7ms/sample - loss: 0.1367 - val_loss: 0.4402\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 343s 7ms/sample - loss: 0.1354 - val_loss: 0.4450\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 339s 7ms/sample - loss: 0.1341 - val_loss: 0.4459\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 337s 7ms/sample - loss: 0.1328 - val_loss: 0.4478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1633a4254c8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x=[encoder_input, decoder_input], y = decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq 기계 번역기 동작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력한 문장에 대해 기계 번역을 하도록 모델을 조정\n",
    "- 전체적인 번역 동작\n",
    "    1. 번역하고자 하는 입력 문장이 인코더에 들어가서 인닉 상태와 셀 상태를 얻는다.\n",
    "    2. 상태와 sos에 해당하는 '\\t'를 디코더에 보낸다.\n",
    "    3. 디코더가 eos에 해당하는 '\\n\\이 나올 때까지 다음 문자를 예측하는 행동을 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-39-5f5c1f8d1edd>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-39-5f5c1f8d1edd>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    decoder_model = Model(inputs=[decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\u001b[0m\n\u001b[1;37m                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "decoder_state_input_h = Input(shape=(256, ))\n",
    "decoder_state_input_c = Input(shape=(256, ))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "encoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_state_inputs, outputs[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input[seq_index:seq_index+1]\n",
    "    decoded_sentence = decoded_sequence(input_seq)\n",
    "    print(35 * '-')\n",
    "    print('입력 문장 : {}'.format(lines.src[seq_index]))\n",
    "    print('정답 문장 : {}'.format(lines.tar[seq_index][1:len(lines.tar[seq_index]) - 1]))\n",
    "    print('번역기가 번역한 문장 : {}'.format(decoded_sentence[:len(decode_sentence) - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 매커니즘(Attention Mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq 모델 : 인코더(encoder)에서 입력 시퀀스를 컨텍스트 벡터(context vector)라는 하나의 고정된 크기의 벡터 표현으로 압축하고, 디코더(decoder)는 이 컨텍스트 벡터(context vector)를 통해서 출력 시퀀스를 만들어내는 모델\n",
    "\n",
    "- seq2seq 모델의 문제점\n",
    "    1. 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하다보니 정보 손실 문제 발생\n",
    "    2. RNN의 고질적 문제인 기울기 소실(Vanishing Gradient) 문제가 존재\n",
    "    \n",
    "- seq2seq 모델의 문제점은 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상이 나타난다.\n",
    "- 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위한 기법이 어텐션 매커니즘(Attention Mechanism)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
